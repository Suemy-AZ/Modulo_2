# -*- coding: utf-8 -*-
"""M2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Suemy-AZ/Modulo_2/blob/main/M2.ipynb
"""

# Commented out IPython magic to ensure Python compatibility.
#Suemy Aquino Zumaya
#A00828585
#08/2022
#Módulo 2

from google.colab import drive  # importa libreria
drive.mount("/content/gdrive");  # utiliza comando
#!pwd
#put your own path in google drive
# %cd "/content/gdrive/MyDrive/7mo semestre/AD2022"
#!ls
import pandas as pd # importar libreria
columns = ["class","alcohol","malic","ash", "alcalinity","Mg","tphenols","flav","nonflav","proantho","color","hue","diluted","proline"]
# Open the file and create the data frame
df = pd.read_csv('wine.data',names = columns)
df

"""Ya que se tienen 3 clases se realizarán un ejercicio de clasificación donde se definirá una ecuación por cada clase con el fin de obtener la probabilidad según la o las características que se estudien eligiendo la más alta como clase final.

Se probará el caso de orden 1 y 2 con una ecuación de hipótesis de transformación de la regresión lineal y una función de costo log-loss además de que se utilizará gradiente descendiente para la obtimización de los coeficientes.

## Definiendo las funciones
"""

##### Orden 1 ####
def orden1 (x_train,y_train,theta,alpha,h,j_i):
  n=len(y_train)
  for idx in range (1000):
    zDelta =[]
    zDeltaX=[]
    for x_i, y_i in zip (x_train,y_train):
      zDelta.append(h(x_i,theta)-y_i)
      zDeltaX.append ((h(x_i,theta)-y_i)*x_i)

    sJt0=sum(zDelta)
    sJt1=sum(zDeltaX)
   #Gradiente descendiente para optimización de theta
    theta[0]= theta[0]-alpha/n*sJt0 
    theta[1]=theta[1]-alpha/n*sJt1
  print(theta)
  return theta

##### Orden 2 #####
def orden2 (x_train,x2_train,y_train,theta,alpha,h,j_i):
  n=len(y_train)
  for idx in range (1000):
    zDelta =[]
    zDeltaX=[]
    zDeltaX2=[]
    
    for x_i, y_i,x2_i in zip (x_train,y_train,x2_train):
      zDelta.append(h(x_i,x2_i,theta)-y_i)
      zDeltaX.append ((h(x_i,x2_i,theta)-y_i)*x_i)
      zDeltaX2.append ((h(x_i,x2_i,theta)-y_i)*x2_i)

    sJt0=sum(zDelta)
    sJt1=sum(zDeltaX)
    sJt2=sum(zDeltaX2)
   #Gradiente descendiente para optimización de theta
    theta[0]= theta[0]-alpha/n*sJt0
    theta[1]=theta[1]-alpha/n*sJt1
    theta[2]=theta[2]-alpha/n*sJt2
  print(theta)
  return theta

#### Validar la theta obtenida Orden 2 ####
def validate2(x_c1t,x2t,y_c1t,x_c1v,x2v,y_c1v,theta,j2_i):
  n_t = len(y_c1v)
  n_v = len(y_c1t)

  # Validación
  zDelta = []
  for x_i, y_i, x2_i in zip(x_c1t,x2t,y_c1t):
    zDelta.append(j2_i(x_i,x2_i,y_i,theta))  

  sDelta = sum(zDelta)  
  J_validate = 1/(2*n_v)*sDelta


  # Entrenamiento
  zDelta = []
  for x_i, y_i,x2_i in zip(x_c1v,y_c1v,x2v):
    zDelta.append(j2_i(x_i,x2_i,y_i,theta))  

  sDelta = sum(zDelta)  
  J_train = 1/(2*n_t)*sDelta

  print(J_validate)
  print(J_train)

#### Validar la theta obtenida Orden 2 ####

def validate(x_c1t,y_c1t,x_c1v,y_c1v,theta):
  n_train = len(y_c1v)
  n_val = len(y_c1t)

  # Validación
  zDelta = []
  for x_i, y_i in zip(x_c1t,y_c1t):
    zDelta.append(j_i(x_i,y_i,theta))  

  sDelta = sum(zDelta)  
  J_validate = 1/(2*n_val)*sDelta


  # Entenamiento
  zDelta = []
  for x_i, y_i in zip(x_c1v,y_c1v):
    zDelta.append(j_i(x_i,y_i,theta))  

  sDelta = sum(zDelta)  
  J_train = 1/(2*n_train)*sDelta

  print(J_validate)
  print(J_train)

"""# Implementación del algoritmo para la característica "malic"


"""

import numpy as np
x= df[["malic"]].to_numpy()
y= df[["class"]].to_numpy()

from sklearn.model_selection import train_test_split

#Se realizan la transformación a 0 y 1 dependiendo de la clase a la que pertenezca

c1=np.where(df["class"] == 1, 1,0 )
c2=np.where(df["class"] == 2, 1,0 )
c3=np.where(df["class"] == 3, 1,0 )

#Se dividen los datos entre entrenamiento y pruebas
x_c1t, x_c1v, y_c1t, y_c1v = train_test_split(x,c1,random_state=0)
x_c2t, x_c2v, y_c2t, y_c2v = train_test_split(x,c2,random_state=0)
x_c3t, x_c3v, y_c3t, y_c3v = train_test_split(x,c3,random_state=0)

"""## Orden 1"""

from math import e,log
h   = lambda x,theta: 1/(1+e**(-(theta[0]+theta[1]*x))) #Función de hipótesis (orden 1)
j_i = lambda x,y,theta: y*log(h(x,theta),e) + (1-y)*log(1-h(x,theta),e) #Función de costo

print("Theta clase 1")
theta11=orden1(x_c1t,y_c1t,[1,1],0.1,h,j_i)
print("Theta clase 2")
theta21=orden1(x_c2t,y_c2t,[1,1],0.1,h,j_i)
print("Theta clase 3")
theta31=orden1(x_c3t,y_c3t,[1,1],0.1,h,j_i)
print("\n")
print("Validación clase 1 [train test]")
validate(x_c1t,y_c1t,x_c1v,y_c1v,theta11)
print("Validación clase 2 [train test]")
validate(x_c2t,y_c2t,x_c2v,y_c2v,theta21)
print("Validación clase 3 [train test]")
validate(x_c3t,y_c3t,x_c3v,y_c3v,theta31)

"""### Visualización de la curva obtenida por cada clase"""

import matplotlib.pyplot as pt

pt.scatter(x,c1)
pt.scatter(x,h(x,theta11))

pt.title("Clase 1 - Orden 1")

pt.scatter(x,c2)
pt.scatter(x,h(x,theta21))
pt.title("Clase 2 - Orden 1")

pt.scatter(x,c3)
pt.scatter(x,h(x,theta31))
pt.title("Clase 3 - Orden 1")

"""### Pruebas de predicción"""

#Prueba Clase 1
print("Predicción clase 1")
x0=x[50]  
y0=y[50]
prob= [h(x0,theta11),h(x0,theta21),h(x0,theta31)]

print("Valores x1: ",x0)
print("Probabilidad por clase",h(x0,theta11),h(x0,theta21),h(x0,theta31))

if max(prob)==prob[0]:
  pred=1
elif max(prob)==prob[1]:
  pred=2
else:
  pred=3

print("Clase predicción: ",pred)
print("Clase real: ",y0,"\n")

#Prueba Clase 2
print("Predicción clase 2")
x0=x[75]  
y0=y[75]
prob= [h(x0,theta11),h(x0,theta21),h(x0,theta31)]

print("Valores x1: ",x0)
print("Probabilidad por clase",h(x0,theta11),h(x0,theta21),h(x0,theta31))

if max(prob)==prob[0]:
  pred=1
elif max(prob)==prob[1]:
  pred=2
else:
  pred=3

print("Clase predicción: ",pred)
print("Clase real: ",y0,"\n")

#Prueba Clase 3
print("Predicción clase 3")
x0=x[150]  
y0=y[150]
prob= [h(x0,theta11),h(x0,theta21),h(x0,theta31)]

print("Valores x1: ",x0)
print("Probabilidad por clase",h(x0,theta11),h(x0,theta21),h(x0,theta31))

if max(prob)==prob[0]:
  pred=1
elif max(prob)==prob[1]:
  pred=2
else:
  pred=3

print("Clase predicción: ",pred)
print("Clase real: ",y0,"\n")

"""Predice parcialmente con probabilidades muy cercanas por lo que se espera que pueda mejorar con una ecuación de orden 2

# Implementación del algoritmo para la característica "malic" y "flav"

## Orden 2
"""

#Definir variable x2 como la característica
import numpy as np
from sklearn.model_selection import train_test_split
x2= (df[["flav"]].to_numpy())

xx= np.concatenate((x, x2), axis=1)

#Dividir los datos entre entrenamiento y pruebas

x_c1t, x_c1v, y_c1t, y_c1v = train_test_split(xx,c1,random_state=0)
x1_c1t_o2=x_c1t[:,0] 
x2_c1t_o2=x_c1t[:,1]
x1_c1v_o2=x_c1v[:,0]
x2_c1v_o2=x_c1v[:,1]

x_c2t, x_c2v, y_c2t, y_c2v = train_test_split(xx,c2,random_state=0)
x1_c2t_o2=x_c2t[:,0]   
x2_c2t_o2=x_c2t[:,1]
x1_c2v_o2=x_c2v[:,0]      
x2_c2v_o2=x_c2v[:,1]

x_c3t, x_c3v, y_c3t, y_c3v = train_test_split(xx,c3,random_state=0)
x1_c3t_o2=x_c3t[:,0]       
x2_c3t_o2=x_c3t[:,1]
x1_c3v_o2=x_c3v[:,0]     
x2_c3v_o2=x_c3v[:,1]

h2   = lambda x,x2,theta: 1/(1+e**(-(theta[0]+theta[1]*x+theta[2]*x2))) #Ecuación de hipótesis (orden 2)
j2_i = lambda x,x2,y,theta: y*log(h2(x,x2,theta),e) + (1-y)*log(1-h2(x,x2,theta),e) #Función de costo

#Obtener theta de cada clase
print("Thetas clase 1")
theta12=orden2(x1_c1t_o2,x2_c1t_o2,y_c1t,[1,1,1],0.1,h2,j2_i) 
print("Thetas clase 2")
theta22=orden2(x1_c2t_o2,x2_c2t_o2,y_c2t,[1,1,1],0.1,h2,j2_i)
print("Thetas clase 3")
theta32=orden2(x1_c3t_o2,x2_c3t_o2,y_c3t,[1,1,1],0.1,h2,j2_i)
print("\n")

#Validar cada theta obtenida
print("Validación clase 1 [train test]")
validate2(x1_c1t_o2,x2_c1t_o2,y_c1t,x1_c1v_o2,x2_c1v_o2,y_c1v,theta12,j2_i)


print("Validación clase 2 [train test]")
validate2(x1_c2t_o2,x2_c2t_o2,y_c2t,x1_c2v_o2,x2_c2v_o2,y_c2v,theta22,j2_i)


print("Validación clase 3 [train test]")
validate2(x1_c3t_o2,x2_c3t_o2,y_c3t,x1_c3v_o2,x2_c3v_o2,y_c3v,theta32,j2_i)

"""### Visualización de la curva obtenida por cada clase"""

import matplotlib.pyplot as pt

pt.scatter(x2,c1)
pt.scatter(x2,h2(x,x2,theta12))
pt.title("Clase 1 - Orden 2")

pt.scatter(x2,c2)
pt.scatter(x2,h2(x,x2,theta22))
pt.title("Clase 2 - Orden 2")

pt.scatter(x2,c3)
pt.scatter(x2,h2(x,x2,theta32))

pt.title("Clase 3 - Orden 2")

"""### Pruebas de predicción"""

#Prueba Clase 1
print("Predicción clase 1")
x0=x[50]  
x02=x2[50]
y0=y[50]
prob= [h2(x0,x02,theta12),h2(x0,x02,theta22),h2(x0,x02,theta32)]

print("Valores x1: ",x0,",x2: ",x02)
print("Probabilidad por clase",h2(x0,x02,theta12),h2(x0,x02,theta22),h2(x0,x02,theta32))

if max(prob)==prob[0]:
  pred=1
elif max(prob)==prob[1]:
  pred=2
else:
  pred=3

print("Clase predicción: ",pred)
print("Clase real: ",y0,"\n")

#Prueba Clase 2
print("Predicción clase 2")
x0=x[75]  
x02=x2[75]
y0=y[75]
prob= [h2(x0,x02,theta12),h2(x0,x02,theta22),h2(x0,x02,theta32)]

print("Valores x1: ",x0,",x2: ",x02)
print("Probabilidad por clase",h2(x0,x02,theta12),h2(x0,x02,theta22),h2(x0,x02,theta32))

if max(prob)==prob[0]:
  pred=1
elif max(prob)==prob[1]:
  pred=2
else:
  pred=3

print("Clase predicción: ",pred)
print("Clase real: ",y0,"\n")

#Prueba Clase 3
print("Predicción clase 3")
x0=x[150]  
x02=x2[150]
y0=y[150]
prob= [h2(x0,x02,theta12),h2(x0,x02,theta22),h2(x0,x02,theta32)]

print("Valores x1: ",x0,",x2: ",x02)
print("Probabilidad por clase",h2(x0,x02,theta12),h2(x0,x02,theta22),h2(x0,x02,theta32))

if max(prob)==prob[0]:
  pred=1
elif max(prob)==prob[1]:
  pred=2
else:
  pred=3

print("Clase predicción: ",pred)
print("Clase real: ",y0,"\n")

"""Se logró predecir correctamente los casos dados con diferencias notorias en las probabilidades individuales por clase."""